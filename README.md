# Regularization
Regularization review (DeepLearning AI)

Regularization is used to reduce overfitting.


<img width="475" alt="스크린샷 2020-06-05 오후 3 43 15" src="https://user-images.githubusercontent.com/48948578/83845168-4d50a680-a743-11ea-8e2f-a1b5abd5d15a.png">

파란 점이랑 빨간 점 나누는 문제.


1. No-Regularization

<img width="525" alt="스크린샷 2020-06-05 오후 3 41 25" src="https://user-images.githubusercontent.com/48948578/83845120-39a54000-a743-11ea-9f54-5d2ea5645200.png">

<img width="517" alt="스크린샷 2020-06-05 오후 3 41 36" src="https://user-images.githubusercontent.com/48948578/83845088-2c885100-a743-11ea-871a-44d2f6341889.png">

    Overfitting 발생 
    
2. L2-Regularization

<img width="552" alt="스크린샷 2020-06-05 오후 3 46 57" src="https://user-images.githubusercontent.com/48948578/83845455-e089dc00-a743-11ea-8fe1-b2fd3776ea7d.png">


<img width="480" alt="스크린샷 2020-06-05 오후 3 47 17" src="https://user-images.githubusercontent.com/48948578/83845460-e1bb0900-a743-11ea-80a1-62491393b913.png">

  Overfitting 감소.
  
<img width="1026" alt="스크린샷 2020-06-05 오후 3 48 50" src="https://user-images.githubusercontent.com/48948578/83845540-10d17a80-a744-11ea-9c6f-ca487e5879d8.png">



3. Drop-Out

<img width="497" alt="스크린샷 2020-06-05 오후 3 50 04" src="https://user-images.githubusercontent.com/48948578/83845729-69087c80-a744-11ea-9d15-e7bc81ef3639.png">


<img width="484" alt="스크린샷 2020-06-05 오후 3 50 10" src="https://user-images.githubusercontent.com/48948578/83845738-6efe5d80-a744-11ea-9dbd-50507795dbbe.png">

<img width="1012" alt="스크린샷 2020-06-05 오후 3 50 21" src="https://user-images.githubusercontent.com/48948578/83845791-86d5e180-a744-11ea-931e-62e874c33c0d.png">




4. Conclusion

<img width="997" alt="스크린샷 2020-06-05 오후 3 50 28" src="https://user-images.githubusercontent.com/48948578/83845798-8a696880-a744-11ea-8802-64a9fcc662eb.png">



5. Program explanation
    -NN model in main.py is 3-layed NN.
    -In the bottom of the main.py there is a part where you can train a model.
    -By changing the traing part you can try out L2, Drop-Out, and No-Regularization.
    
   





